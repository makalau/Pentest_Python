import requests
from bs4 import BeautifulSoup

    
def inicio():
    to_craw = list() #Essa variável armazenará as urls que será feito o WebCrawler
    crawled = set() #Essa variável armazenará as urls que já foi feito o WebCrawler
    to_craw.append("http://www.example.com")
    
    while True:
        if to_craw:
            url = to_craw.pop()
            response = requests.get(url)
            links = get_links(response.text)
            if links:
                for link in links:
                    if link not in crawled and link not in to_craw:
                        if link.startswith("http"):
                            links.append(link)
                        
                print(f"Crawling: {to_craw}")
        else:
            break
        
        
def get_links(html):
    links = list()
    try:
        soup = BeautifulSoup(html, "html.parser")
        tags_a = soup.find_all("a")
        for tag in tags_a:
            links.append(tag["href"])
        return links
    except:
        print("Ocorreu um erro.")


inicio()